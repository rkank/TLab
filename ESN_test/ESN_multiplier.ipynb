{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reservoir\n",
    "Here I give a walkthrough of how to use PyTorch for constructing a reservoir, essentially a randomized recurrent neural network, for classification. A more in-depth implementation which could be good for reference is [EchoTorch](https://github.com/nschaetti/EchoTorch)\n",
    "\n",
    "The main steps I use are to initialize a RNN with standard nodes, replace the weight matrix only the readout layer is trained.\n",
    "\n",
    "There are many other parameters to tune."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as utils\n",
    "\n",
    "\n",
    "# set random seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First in order to use PyTorch effectively we want to it up to use the availabe GPU device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Setup the ability to run on a GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constructing the reservoir\n",
    "Here we create a recurrent neural network, set its internal weights according to the Echo State Property and then freeze all the weights except the output.\n",
    "Additionally aother simple way of doing this is by removing the final layer and running the data through the reservoir and then training a classifier on these node output values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class ESN(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden):\n",
    "        super(ESN, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.hidden = nn.RNN(input_size, n_hidden, batch_first=True, bias=False)\n",
    "        self.fc = nn.Linear(n_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(1, x.size(0), self.n_hidden).to(device)\n",
    "\n",
    "        # Forward propagate\n",
    "        out, h_n = self.hidden(x, h0)  # out: tensor of shape (batch_size, seq_length, n_hidden)\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out\n",
    "\n",
    "def create_reservoir_matrix(size=(10,10), spectral_radius=0.9, sparsity=0.5):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    size: square matrix representing the size of teh reservoir connections\n",
    "    spectral_radius: largest eigenvalue in reservoir matrix should be <1\n",
    "    sparsity: connectivity of matrix, 1.0 indicates full connection\n",
    "    \"\"\"\n",
    "    # generate uniformly random from U[0,1] *2 -1 makes it U[-1,1]\n",
    "    W_res = torch.rand(size)*2-1\n",
    "\n",
    "    # create a sparse mask array then multiply reservoir weights by sparse mask\n",
    "    # sparse mask is done by generating uniform[0,1] then setting True <=sparsity\n",
    "    W_res = W_res*(torch.rand_like(W_res) <= sparsity).type(W_res.dtype)\n",
    "\n",
    "    # scale the matrix to have the desired spectral radius\n",
    "    W_res = W_res*spectral_radius/(np.max(np.abs(np.linalg.eigvals(W_res))))\n",
    "\n",
    "    return W_res\n",
    "\n",
    "\n",
    "def epoch_accuracy(loader, total_size, split='train'):\n",
    "    \"\"\"\n",
    "    evaluates the accuracy of the model on the dataset\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Measure the accuracy for the entire dataset\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        outputs = model(inputs)\n",
    "        predicted = inputs.prod(0)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == outputs).sum()\n",
    "\n",
    "    print(f'Accuracy of the model on the {total_size} {split} sequences: {float(correct) / total:3.1%}')\n",
    "\n",
    "    return float(correct) / total\n",
    "\n",
    "class MultDataset(Dataset):\n",
    "\tdef __init__(self, size):\n",
    "\t\tself.size = size\n",
    "\t\tself.data = np.random.random((size, 2))*2-1\n",
    "\t\tself.data = np.hstack((self.data, np.multiply(self.data[:, 0], self.data[:, 1]).reshape((size, 1))))\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.size\n",
    "\n",
    "\tdef __getitem__(self, item):\n",
    "\t\tif torch.is_tensor(item):\n",
    "\t\t\titem = item.tolist()\n",
    "\n",
    "\t\treturn torch.from_numpy(self.data[item, :2]), torch.from_numpy(np.asarray(self.data[item, 2]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Hyper Parameters\n",
    "num_epochs = 10\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1024\n",
    "learning_rate = 1e-3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "train_dataset = MultDataset(1024)\n",
    "test_dataset = MultDataset(1024)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size_test, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# input parameters\n",
    "input_size = 2\n",
    "\n",
    "# Hyper-parameters\n",
    "n_hidden = 50\n",
    "batch_size = batch_size_train\n",
    "learning_rate = 0.001\n",
    "\n",
    "# create the internal weight matrix\n",
    "W_res = create_reservoir_matrix(size=(n_hidden, n_hidden), spectral_radius=0.9, sparsity=1.0)\n",
    "\n",
    "model = ESN(input_size, n_hidden)\n",
    "\n",
    "# set the internal hidden weight matrix as the reservoir values\n",
    "model.hidden.weight_hh_l0 = nn.Parameter(W_res, requires_grad=False)\n",
    "input_scale = 0.5\n",
    "model.hidden.weight_ih_l0 = nn.Parameter((torch.rand((n_hidden,input_size))*2-1)*input_scale, requires_grad=False)\n",
    "\n",
    "# move to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Freeze all hidden layers so no gradient update occurs\n",
    "for param in model.hidden.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# only need the gradient for the fully connected layer, weight_decay adds l2 norm\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# CrossEntropyLoss takes care of the Softmax evaluation\n",
    "criterion = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (<ipython-input-33-6a7d0d85374a>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"<ipython-input-33-6a7d0d85374a>\"\u001B[1;36m, line \u001B[1;32m29\u001B[0m\n\u001B[1;33m    if epoch==0 or (epoch+1)%5==0:\u001B[0m\n\u001B[1;37m                                  ^\u001B[0m\n\u001B[1;31mTabError\u001B[0m\u001B[1;31m:\u001B[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # reshape to (batch_size, time_step, input size)\n",
    "        inputs = inputs.view(1,batch_size_train,input_size)\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "plt.plot(train_acc)\n",
    "plt.plot(test_acc)\n",
    "plt.legend(['train_acc','test_acc'])#%%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}